{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Meet Ranger","text":"Note <p>Ranger is under active development, and the documentation is a work in progress. We welcome contributions and feedback to improve it. \ud83d\ude0d</p> <p>Ranger is AI infrastructure for cyber range work. It helps you automate the boring stuff, orchestrate the complicated stuff, and build capabilities that are more realistic, scale more easily, and provide increased value.</p> <p>Whether you're running a training range, building a scenario, or experimenting with AI-enabled operations, Ranger acts as your range co-pilot: Planning, deploying, and dynamically adapting different areas of the exercise in real time using an open-source (OSS) and modular generative AI stack.</p> Run Ranger on the Crucible Framework <p> Ranger runs alongside the SEI's Crucible Framework, a modular system for creating and managing virtual environments for training and exercises. Find more on Crucible's source code and Crucible Docs.</p> <p>Ranger is also designed to drive GHOSTS NPC agents in simulating all manner of user behavior in your range. Learn more about GHOSTS.</p>"},{"location":"#why-we-built-ranger","title":"Why We Built Ranger","text":"<p>Cyber ranges are awesome, but let\u2019s face it, they can be difficult to manage. Scenario setup is tedious. Injecting new events takes a human. Exercises feel like plays stuck in rehearsal: Scripted, brittle, and already a bit outdated the moment they run.</p> <p>Ranger hopes to fix that. It\u2019s an intelligent layer that sits on top of your range tooling and makes things actually dynamic. It looks to:</p> <ul> <li>Drive live injects based on trainee behavior</li> <li>Adapt to how teams are performing</li> <li>Automate red/blue/gray actions in real time</li> <li>Scale without instructors watching every move</li> </ul>"},{"location":"#what-ranger-actually-does","title":"What Ranger Actually Does","text":"<p>Ranger is more than a chatbot. Here\u2019s how it works in practice:</p> <ol> <li>You describe what you want. Natural language, no scripting required. \"I want ghosts agents to set off a phishing campaign after the team resets the firewall.\" Cool, noted.</li> <li>Ranger asks questions. If your description is unclear to Ranger, it asks follow up questions, checks what\u2019s possible, and proposes a course of action.</li> <li>It acts on your behalf. It actually executes commands: Launches NPC activity through GHOSTS, connects services, and injects network events.</li> </ol> <p>All of this works without references to cloud services and can utilize open-weight models, so you can use Ranger in secure or air-gapped environments.</p>"},{"location":"#why-use-ranger","title":"Why Use Ranger?","text":"<p>Here\u2019s what you get when you bring Ranger into the mix:</p> <ul> <li>Faster scenario creation \u2014 From hours of YAML to a few lines of conversation</li> <li>Live, adaptive exercises \u2014 Trainees act, Ranger responds</li> <li>Realistic content \u2014 Fake news, social media, email traffic, user behavior, and more</li> <li>No vendor lock-in \u2014 Runs locally, supports open models, speaks API</li> <li>Lowers the barrier to entry \u2014 You shouldn't need to be a range ninja or an LLM whisperer</li> </ul>"},{"location":"quickstart-appliance/","title":"Ranger Appliance Quickstart Guide","text":"<p>This guide will help you get a Ranger instance up and running as a VM on Proxmox.</p>"},{"location":"quickstart-appliance/#requirements","title":"Requirements","text":"<ul> <li>Proxmox VE 7.4 or later</li> <li>Packer installed locally</li> <li>At least 16GB RAM recommended</li> <li>Internet access (initial build and image pull only)</li> </ul>"},{"location":"quickstart-appliance/#clone-configure-and-build-the-appliance","title":"Clone, configure, and build the Appliance","text":"<pre><code>git clone https://github.com/CMU-SEI/RangerAI.git\ntouch .env\n</code></pre> <p>This has created an .env file in the root of the repo. Now add the following content:</p> <pre><code>BASEROW_PUBLIC_URL=\"\" set to your machine\u2019s IP or domain\nOLLAMA_HOST=\"http://localhost:11434\" #Ollama server URL\nN8N_API_URL=\"http://localhost:5678\" #n8n API URL\nN8N_API_KEY=\"\" #set to your n8n API key\nGHOSTS_HOST=\"\" #set to your ghosts api url\n</code></pre> <p>Edit any port conflicts by changing host:container mappings in <code>docker-compose.yml</code> if needed.</p> <p>Lastly, start the Packer build process:</p> <pre><code>cd ranger-appliance\npacker build ranger.pkr.hcl\n</code></pre> <p>This launches:</p> <ul> <li>Docs on http://localhost:8888</li> <li>Open-WebUI (chat frontend) on port 3001</li> <li>Ranger API on port 5076</li> <li>n8n (workflow engine) on port 5678</li> <li>Qdrant (vector database) on port 6333</li> <li>Baserow (data backend) on port 80</li> <li>Ghosts MCP (tool server) on port 8000</li> </ul>"},{"location":"quickstart-appliance/#start-ollama","title":"Start Ollama","text":"<p>If you haven't already, install Ollama and run it:</p> <pre><code>ollama serve\n</code></pre> <p>You may need to pull models. For example, to pull the Mistral model:</p> <pre><code>ollama pull mistral:latest\n</code></pre>"},{"location":"quickstart-appliance/#configure-the-workflow-interface","title":"Configure the Workflow Interface","text":"<ol> <li>Open a browser and navigate to: <code>http://localhost:5678</code>. If it does not render, run <code>docker restart n8n</code>.</li> <li>Create your admin account. This is your main interface for interacting with Ranger as an Admin.</li> <li>Click on your account name and go to settings. Create an n8n API Key. Copy it into your .env file.</li> <li>Import any workflows and complete their necessary accounts for access to second-level tools such as Qdrant, etc.</li> <li>Rebuild Ranger: <code>docker compose up -d --force-recreate ranger</code> - Ranger will poll n8n for active workflows now.</li> </ol>"},{"location":"quickstart-appliance/#settings-up-client-access-to-test-the-system","title":"Settings Up Client Access to Test the System","text":"<p>Open a browser and navigate to: <code>http://localhost:3001</code></p> <ol> <li>Setup a new user account.</li> <li>Go to admin settings and add a connection to both Ollama and Ranger, using the urls http://host.docker.internal:11434 for Ollama and http://host.docker.internal:5076 for Ranger.</li> <li>Now return to the main page and switch the chat model to a Ranger model (e.g., <code>ranger:mistral</code>).</li> </ol> <p>Try a prompt like:</p> <pre><code>ghosts machines list\n</code></pre> <p>Ranger will parse, plan, and execute using GHOSTS or other tools via MCP.</p>"},{"location":"quickstart/","title":"Ranger Docker Quickstart Guide","text":"<p>This guide will help you get a local Ranger instance up and running quickly via Docker Compose.</p>"},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Docker and Docker Compose v2+</li> <li>At least 16GB RAM recommended</li> <li>Internet access (initial image pull only)</li> </ul>"},{"location":"quickstart/#clone-the-repo","title":"Clone the Repo","text":"<p>Start the stack:</p> <pre><code>git clone https://github.com/CMU-SEI/RangerAI.git\ncd RangerAI\n</code></pre>"},{"location":"quickstart/#update-configuration","title":"Update Configuration","text":"<p>Create an .env file in the root of the repo with the following content:</p> <pre><code>BASEROW_PUBLIC_URL=\"\" set to your machine\u2019s IP or domain\nOLLAMA_HOST=\"http://localhost:11434\" #Ollama server URL\nN8N_API_URL=\"http://localhost:5678\" #n8n API URL\nN8N_API_KEY=\"\" #set to your n8n API key\nGHOSTS_HOST=\"\" #set to your ghosts api url\n</code></pre> <p>Edit any port conflicts by changing host:container mappings in <code>docker-compose.yml</code> if needed.</p>"},{"location":"quickstart/#start-the-stack","title":"Start the Stack","text":"<pre><code>docker compose up -d\n</code></pre> <p>This launches:</p> <ul> <li>Open-WebUI (chat frontend) on port 3001</li> <li>Ranger API on port 5076</li> <li>n8n (workflow engine) on port 5678</li> <li>Qdrant (vector database) on port 6333</li> <li>Baserow (data backend) on port 80</li> <li>Ghosts MCP (tool server) on port 8000</li> </ul>"},{"location":"quickstart/#start-ollama","title":"Start Ollama","text":"<p>If you haven't already, install Ollama and run it:</p> <pre><code>ollama serve\n</code></pre> <p>You may need to pull models. For example, to pull the Mistral model:</p> <pre><code>ollama pull mistral:latest\n</code></pre>"},{"location":"quickstart/#configure-the-workflow-interface","title":"Configure the Workflow Interface","text":"<ol> <li>Open a browser and navigate to: <code>http://localhost:5678</code>. If it does not render, run <code>docker restart n8n</code>.</li> <li>Create your admin account. This is your main interface for interacting with Ranger as an Admin.</li> <li>Click on your account name and go to settings. Create an n8n API Key. Copy it into your .env file.</li> <li>Import any workflows and complete their necessary accounts for access to second-level tools such as Qdrant, etc.</li> <li>Rebuild Ranger: <code>docker compose up -d --force-recreate ranger</code> - Ranger will poll n8n for active workflows now.</li> </ol>"},{"location":"quickstart/#settings-up-client-access-to-test-the-system","title":"Settings Up Client Access to Test the System","text":"<p>Open a browser and navigate to: <code>http://localhost:3001</code></p> <ol> <li>Setup a new user account.</li> <li>Go to admin settings and add a connection to both Ollama and Ranger, using the urls http://host.docker.internal:11434 for Ollama and http://host.docker.internal:5076 for Ranger.</li> <li>Now return to the main page and switch the chat model to a Ranger model (e.g., <code>ranger:mistral</code>).</li> </ol> <p>Try a prompt like:</p> <pre><code>ghosts machines list\n</code></pre> <p>Ranger will parse, plan, and execute using GHOSTS or other tools via MCP.</p>"},{"location":"quickstart/#stop-the-stack","title":"Stop the Stack","text":"<pre><code>docker compose down\n</code></pre> <p>Volumes persist between runs. To clean everything:</p> <pre><code>docker compose down -v\n</code></pre>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>This document walks you through everything needed to install and configure Ranger \u2014 from prerequisites to smoke testing. Whether you're setting up in a connected lab or a fully air-gapped environment, you'll get Ranger running cleanly and confidently.</p> <p>Ranger uses a modular architecture built around Docker containers. It plays well with most range infrastructure and is designed to scale \u2014 both technically and in terms of scenario complexity.</p> <ol> <li> <p>System Requirements and Prerequisites</p> <p>You\u2019ll need:</p> <ul> <li>A Linux \ud83d\udc27 (or MacOS \ud83c\udf4e) host (VM or bare metal) with:<ul> <li>Docker \ud83d\udc33</li> <li>At least 16GB RAM (more for large LLMs)</li> <li>NVIDIA GPU + drivers for the LLM (or Apple Silicon on MacOS)</li> </ul> </li> <li>Docker Compose (optional, but hugely helpful)</li> <li>Ollama installed on the host (or included via container)</li> <li>For air-gapped deployment:<ul> <li>An internet-connected machine to fetch Docker images and LLM model files</li> <li>A secure way to move files to your isolated target system</li> </ul> </li> </ul> </li> <li> <p>Creating the Docker Network <pre><code>docker network create ranger-net\n\ndocker run -d --name n8n --network ranger-net -p 5678:5678 \\\n  -v n8n_data:/home/node/.n8n \\\n  -e N8N_SECURE_COOKIE=false \\\n  --restart unless-stopped \\\n  docker.n8n.io/n8nio/n8n:1.90.2\n\ndocker run -d --name open-webui --network ranger-net -p 3000:8080 \\\n  -e OPENBLAS_NUM_THREADS=1 -e OMP_NUM_THREADS=1 \\\n  --ulimit nproc=8192 --ulimit nofile=65535:65535 \\\n  -v open-webui-data:/app/backend/data \\\n  --restart always ghcr.io/open-webui/open-webui:main\n\ndocker run -d --name pipelines --network ranger-net -p 9099:9099 \\\n  --restart always ghcr.io/open-webui/pipelines:main\n\ndocker run -d --name qdrant --network ranger-net -p 6333:6333 \\\n  --restart unless-stopped qdrant/qdrant:latest\n\ndocker run -d --name baserow --network ranger-net -p 8081:80 \\\n  -v baserow_data:/baserow/data \\\n  -e BASEROW_PUBLIC_URL=http://&lt;HOST_IP&gt;:8081 \\\n  --restart unless-stopped baserow/baserow:1.32.5\n</code></pre></p> </li> </ol>"},{"location":"deployment/configuration/","title":"Configuring and Operating Ranger","text":"<p>Once deployment is successful, the next step is understanding how to configure Ranger for your specific exercises and operate it during live runs. This section covers:</p> <ul> <li>Connecting additional range APIs via MCP</li> <li>Orchestrating GHOSTS through Ranger</li> <li>Tuning prompts and logging</li> <li>Using interfaces (chat, CLI, workflows)</li> </ul>"},{"location":"deployment/configuration/#connecting-range-apis-via-mcp","title":"Connecting Range APIs via MCP","text":"<p>Ranger uses the Model Context Protocol (MCP) to integrate with external services. Each service (e.g., VM orchestration, scoring) is wrapped in an MCP server.</p> <p>Steps to add an MCP integration:</p> <ol> <li> <p>Deploy or Enable the MCP Server:</p> </li> <li> <p>Use an existing implementation or generate one with tools like FastMCP.</p> </li> <li> <p>Base it on the service\u2019s OpenAPI (Swagger) spec.</p> </li> <li> <p>Register the MCP Server:</p> </li> <li> <p>Set environment variables (e.g., <code>MCP_GHOSTS_ADDR=ws://ghosts-mcp:8000</code>).</p> </li> <li> <p>Repeat for each service (e.g., <code>MCP_SERVICEX_ADDR=http://servicex-mcp:port</code>).</p> </li> <li> <p>Update Tool Context:</p> </li> <li> <p>Extend Ranger\u2019s system prompt to describe new tools.</p> </li> <li> <p>Include function names and descriptions.</p> </li> <li> <p>Test the Integration:</p> </li> <li> <p>Use a natural prompt (e.g., \"create a new VM\") and check if Ranger calls the MCP function.</p> </li> <li>If it fails, refine tool descriptions or debug connectivity.</li> </ol> <p>Design Tip: Use intuitive aliases for functions. Check open-source MCP definitions for inspiration.</p>"},{"location":"deployment/configuration/#ghosts-orchestration","title":"GHOSTS Orchestration","text":"<p>Ranger simplifies GHOSTS control. Key workflows include:</p> <ul> <li>Initial Setup: Ensure NPC agents connect to the GHOSTS server. Pre-load timeline/scripts.</li> <li>Start NPCs: Example prompt: \"Start 10 NPCs in finance segment doing office work.\"</li> <li>Monitor NPCs: Ask: \"How many NPCs are active and what are they doing?\"</li> <li>Adjust in Real-Time: Prompts like \"Increase browsing NPCs to 20 over the next hour\" are translated to API calls.</li> </ul> <p>The GHOSTS MCP server supports SSE/WebSocket for live command streaming and async updates.</p>"},{"location":"deployment/configuration/#prompt-management-and-logging","title":"Prompt Management and Logging","text":""},{"location":"deployment/configuration/#system-prompts","title":"System Prompts","text":"<ul> <li>Define Ranger\u2019s role, available tools, and policy rules.</li> <li>Keep it concise but authoritative.</li> <li>Optionally include few-shot examples for behavior.</li> </ul>"},{"location":"deployment/configuration/#user-guidance","title":"User Guidance","text":"<ul> <li>Provide a cheat sheet for phrasing requests.</li> <li>Encourage specificity in prompts.</li> </ul>"},{"location":"deployment/configuration/#logging","title":"Logging","text":"<ul> <li>Log all interactions to a database (e.g., Baserow).</li> <li> <p>Review logs for:</p> </li> <li> <p>Misunderstandings</p> </li> <li>Hallucinations</li> <li>Performance</li> <li>User feedback (e.g., from Open-WebUI ratings)</li> </ul>"},{"location":"deployment/configuration/#prompt-adjustments","title":"Prompt Adjustments","text":"<ul> <li>Refine system prompt for recurring issues.</li> <li>Add retry/correction logic in the core.</li> <li>Consider fine-tuning or model switching.</li> <li>Update MCP tool schemas for clarity.</li> </ul>"},{"location":"deployment/configuration/#interfaces","title":"Interfaces","text":""},{"location":"deployment/configuration/#web-chat-open-webui","title":"Web Chat (Open-WebUI)","text":"<ul> <li>Primary interface for most users.</li> <li>Supports multi-chat, permissions, and logs for AAR.</li> </ul>"},{"location":"deployment/configuration/#command-line-cli","title":"Command-Line (CLI)","text":"<ul> <li>Use curl or scripts to call Ranger's OpenAI-compatible API:</li> </ul> <pre><code>curl http://localhost:5067/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"Ranger\", \"messages\": [{\"role\": \"user\", \"content\": \"Summarize current range status.\"}]}'\n</code></pre>"},{"location":"deployment/configuration/#n8n-workflows","title":"n8n Workflows","text":"<ul> <li>Trigger Ranger via HTTP webhooks or schedules.</li> <li>Automate summaries, monitoring, or downstream actions.</li> </ul>"},{"location":"deployment/configuration/#direct-api-integration","title":"Direct API Integration","text":"<ul> <li>Use Ranger as a backend in custom apps.</li> <li>Follows OpenAI API structure, so most SDKs work with just a base URL change.</li> </ul> <p>This configuration and operations layer turns Ranger from a chatbot into a range-integrated orchestrator. Whether you're working via chat, script, or dashboard, the same AI logic applies\u2014making it flexible and operational in any training environment.</p>"},{"location":"deployment/tuning/","title":"Tuning","text":""},{"location":"deployment/tuning/#ranger-testing-and-prompt-adjustment-guide","title":"Ranger Testing and Prompt Adjustment Guide","text":"<p>After deployment and initial configuration, thoroughly test Ranger in a controlled environment before relying on it in critical exercises. This guide outlines a step-by-step process for validating workflows, adjusting prompts, and ensuring stable operations.</p>"},{"location":"deployment/tuning/#step-by-step-workflow-testing","title":"Step-by-Step Workflow Testing","text":"<p>Step 1: Define Test Cases</p> <ul> <li> <p>Identify expected behaviors:</p> </li> <li> <p>Interview and deploy GHOSTS: Ranger gathers needed info, confirms, then launches Ghosts.</p> </li> <li>Dynamic event injection: e.g., \"trigger ransomware on machine X.\"</li> <li>Status queries: Ranger reports current range status.</li> <li>Error handling: How Ranger responds to invalid or malicious input.</li> <li>Write expected outcomes for each.</li> </ul> <p>Step 2: Run Tests via Chat or API</p> <ul> <li>Use Open-WebUI chat for manual testing.</li> <li>For repeatability, script API calls to <code>/v1/chat/completions</code>.</li> <li>Keep each test in a new chat to isolate context.</li> <li>Example: Ask \"Set up 5 NPCs for finance team browsing site X.\" Track follow-ups, confirmations, and Ghosts execution.</li> </ul> <p>Step 3: Record Outputs</p> <ul> <li>Use Baserow logs or export chat transcripts.</li> <li> <p>Note:</p> </li> <li> <p>Logical flow</p> </li> <li>Correct function usage</li> <li>Tone/clarity</li> </ul> <p>Step 4: Introduce Variations</p> <ul> <li>Change input phrasing or omit parameters.</li> <li>See if Ranger prompts for clarification or defaults sensibly.</li> </ul> <p>Step 5: Failure Modes</p> <ul> <li> <p>Test bad input:</p> </li> <li> <p>\"Launch 10000 NPCs\"</p> </li> <li>\"Give me admin password\"</li> <li>Confirm graceful rejection or clarification.</li> </ul> <p>Step 6: Multi-turn and Memory</p> <ul> <li> <p>Check context tracking:</p> </li> <li> <p>\"Start 5 NPCs on Site A.\"</p> </li> <li>Later: \"Now have those users check email.\"</li> <li>If it forgets, consider prompt augmentation or using RAG.</li> </ul> <p>Step 7: Load Testing (Optional)</p> <ul> <li>Simulate concurrent usage with scripts or JMeter.</li> <li>Watch for latency or resource issues.</li> <li>Scale horizontally if needed.</li> </ul>"},{"location":"deployment/tuning/#capturing-results-and-adjusting-prompts","title":"Capturing Results and Adjusting Prompts","text":"<p>Common Issues and Fixes:</p> <ul> <li>Lack of follow-up questions: Refine system prompt with, e.g., \"Ask follow-up questions if user input is underspecified.\"</li> <li>Wrong/missing function calls: Improve function descriptions or add few-shot examples.</li> <li>Awkward phrasing: Adjust prompt tone settings or reword style guidance.</li> <li>Hallucinations: Emphasize factual grounding in prompt. Reduce temperature.</li> <li>Slow responses: Identify bottlenecks (e.g., n8n latency). Use async patterns where possible.</li> </ul> <p>Iterate:</p> <ul> <li>Change prompt/config \u2192 re-run tests \u2192 confirm fix</li> <li>Example: Initial Ghosts deployment failed due to bad JSON. Fix format and prompt = success.</li> </ul>"},{"location":"deployment/tuning/#design-considerations-and-trade-offs","title":"Design Considerations and Trade-offs","text":"<p>Autonomy vs Control:</p> <ul> <li>Decide when to require confirmation. Prompt should reflect policy.</li> </ul> <p>Transparency:</p> <ul> <li>Ensure Ranger explains its actions (\"Calling Ghosts API now...\").</li> <li>Helps build trust and aids debugging.</li> </ul> <p>Security:</p> <ul> <li>Restrict sensitive actions via roles.</li> <li>Tag functions in MCP as \"admin-only.\"</li> </ul> <p>Maintenance:</p> <ul> <li> <p>Ranger\u2019s modularity allows:</p> </li> <li> <p>Swapping LLMs (via Ollama)</p> </li> <li>Updating MCP servers</li> <li>Revalidate behavior after upgrades.</li> </ul> <p>By following this guide, admins and integrators can confidently verify Ranger behavior, fix gaps early, and tune the system to meet operational needs. Effective AI-driven automation requires iterative testing, prompt management, and clarity in intent translation. Ranger's design supports this process and can significantly enhance cyber range realism and control when correctly configured.</p>"},{"location":"infrastructure/","title":"Architecture and Core Components","text":"<p>Ranger\u2019s architecture consists of several integrated components (both existing open-source tools and custom Ranger modules) that together enable its intelligent capabilities. The design emphasizes modularity and offline operation \u2013 each component can be deployed in containers, and all AI inference uses local models right out of the box. Below we describe the core components and integrations in Ranger\u2019s architecture, including how they interact and why they were chosen.</p> <p>The key infrastructure components are:</p> <p></p> <ul> <li>Client Interface \u2013 Web-based chat interface</li> <li>The Ranger API \u2013 The Ranger service</li> <li>Workflows \u2013 automations and task orchestration</li> <li>MCP \u2013 Model Context Protocol (MCP) and tool access</li> <li>RAG \u2013 Retrieval-Augmented Generation (RAG) and vector search setup</li> <li>Data Storage \u2013 Tabular data layer system</li> <li>AI Provider \u2013 Model backend (Ollama or other LLM)</li> </ul>"},{"location":"infrastructure/ai_provider/","title":"Ranger's AI Provider (or model engine)","text":"<p>Ranger\u2019s ability to understand natural language and respond intelligently is powered by it's AI Provider \u2014 it's LLM backend.</p> <p>By default, Ranger uses Ollama for this role \u2014 an open-source model server that can run large language models (LLMs) locally. But the AI Provider is not tied to Ollama: it\u2019s designed to support any compatible LLM backend, including OpenAI, Claude, open-weight models via vLLM, or even internal fine-tuned APIs.</p>"},{"location":"infrastructure/ai_provider/#why-use-ollama","title":"Why Use Ollama?","text":"<p>Ollama was chosen as the default for several reasons:</p> <ul> <li>Local-first: It runs entirely on-prem without internet.</li> <li>Supports many models: LLaMA, Mistral, Phi, and others.</li> <li>Hardware optimized: Runs efficiently on CPUs, Apple Silicon, or GPUs.</li> <li>Simple interface: Exposes a REST API (default: port <code>11434</code>) that Ranger can talk to.</li> <li>Air-gapped friendly: No reliance on cloud APIs or license tokens.</li> </ul> <p>It\u2019s fast, flexible, and easy to switch models or versions depending on your hardware or needs.</p> Note <p>We are always looking to improve Ranger's AI capabilities. If you have a preferred LLM backend or model that you think would enhance Ranger, please open an issue.</p>"},{"location":"infrastructure/ai_provider/#how-the-ai-provider-works","title":"How the AI Provider Works","text":"<p>When a user sends a message \u2014 whether via chat UI, n8n, or CLI \u2014 the flow looks like this:</p> <ol> <li>Prompt comes in from Open-WebUI (or another frontend).</li> <li>Ranger intercepts it via its API, instead of letting the frontend talk to Ollama directly.</li> <li>Ranger evaluates the prompt:</li> <li>If it\u2019s a language-only query (e.g. \u201cCan you generate a python 101 module for data science?\u201d), it forwards it to Ollama and returns the answer.</li> <li>If it\u2019s an actionable command (e.g. \u201cStart 3 NPC agents\u201d), Ranger interprets and executes it using the appropriate GHOSTS workflow.</li> <li>Ranger returns the final result to the user.</li> </ol> <p>This proxy architecture ensures all requests pass through Ranger, allowing it to mediate, enrich, or act on them intelligently.</p>"},{"location":"infrastructure/ai_provider/#swapping-out-the-brain","title":"Swapping Out the Brain","text":"<p>Ranger isn\u2019t married to Ollama. It can talk to:</p> <ul> <li>Any OpenAI-compatible API (e.g. OpenAI, Azure, Groq)</li> <li>Claude (via a proxy or plugin)</li> <li>Local APIs you host yourself</li> <li>Ollama, with any model you install</li> </ul> <p>To switch models in Ollama:</p> <pre><code>ollama list\nollama pull mistral:latest\nollama run mistral\n</code></pre> <p>Or use Open-WebUI\u2019s model selector if running interactively.</p> <p>You can even point Ranger at different model endpoints using environment variables:</p> <pre><code>LLM_API_BASE=http://ollama:11434\nLLM_MODEL=llama2\n</code></pre>"},{"location":"infrastructure/ai_provider/#isolation-by-design","title":"Isolation by Design","text":"<p>A key design decision: LLM execution happens outside Ranger.</p> <p>Ranger never loads models into its own process. Instead, it treats the LLM like an external compute resource \u2014 asking questions, getting completions, and deciding what to do next.</p> <p>This gives you:</p> <ul> <li>Fault isolation (LLM crashes don\u2019t affect Ranger logic)</li> <li>Better performance (no model bloat inside the API)</li> <li>Deployment flexibility (you can swap the model backend at any time)</li> </ul>"},{"location":"infrastructure/ai_provider/#tldr","title":"TL;DR","text":"<p>Ranger\u2019s AI Provider is the pluggable AI layer that handles language understanding. By default it uses Ollama, but it\u2019s backend-agnostic. This keeps Ranger flexible, lightweight, and adaptable to your environment \u2014 whether you\u2019re using open models, fine-tunes, or proprietary APIs.</p> <p>Want to plug in your own brain? Just point Ranger at it.</p>"},{"location":"infrastructure/data_storage/","title":"Data Storage","text":"<p>Ranger uses structured data storage to manage logs, configurations, test results, and workflow history. While the current implementation uses Baserow\u2014an open-source, self-hosted Airtable-style database\u2014the system is flexible and can be adapted to any backend with a RESTful or SQL interface.</p>"},{"location":"infrastructure/data_storage/#why-use-structured-storage","title":"Why Use Structured Storage?","text":"<p>In AI-driven infrastructure, it's not enough to observe behaviors\u2014we need to log, review, and improve them. Data storage supports:</p> <ul> <li>Prompt Iteration: Capturing prompts and completions for analysis and tuning.</li> <li>Workflow Tracking: Logging function calls, responses, and system decisions over time.</li> <li>Test Results: Recording test case outcomes for reproducibility and regression checks.</li> <li>Metadata and Configurations: Storing knowledge index metadata, scenario templates, or environment-specific variables.</li> </ul>"},{"location":"infrastructure/data_storage/#current-implementation-baserow","title":"Current Implementation: Baserow","text":"<p>We currently use Baserow as the backing store because it offers:</p> <ul> <li>Web UI for managing data without needing a full-blown admin interface</li> <li>REST API access for programmatic updates and queries</li> <li>Self-hosted, offline operation, supporting air-gapped deployments</li> <li>Relational structure: linked tables, rich field types, and user roles</li> </ul> <p>By default, we define tables for:</p> <ul> <li><code>Prompts</code>: each message sent to the LLM, with metadata</li> <li><code>Completions</code>: associated responses, duration, tokens, etc.</li> <li><code>FunctionCalls</code>: structured logs of MCP-triggered actions</li> <li><code>Tests</code>: predefined test cases and results</li> <li><code>Sessions</code>: session metadata, timestamps, user ID, etc.</li> </ul>"},{"location":"infrastructure/data_storage/#integrating-with-ranger","title":"Integrating With Ranger","text":"<p>Ranger\u2019s API includes logic to automatically:</p> <ul> <li>Write logs to Baserow during LLM interactions</li> <li>Record which MCP functions were invoked and with what arguments</li> <li>Associate conversation history with test runs</li> </ul> <p>This data can be filtered, exported, or audited later\u2014especially useful in exercises or evaluations.</p>"},{"location":"infrastructure/data_storage/#alternative-backends","title":"Alternative Backends","text":"<p>If Baserow isn\u2019t suitable for your environment, alternatives include:</p> <ul> <li>PostgreSQL or SQLite for direct integration into existing DB infrastructure</li> <li>Airtable (if cloud access is allowed)</li> <li>Custom REST API wrappers over your own storage schema</li> </ul> <p>Ranger does not assume a fixed schema\u2014just that it can log JSON-structured records via HTTP.</p>"},{"location":"infrastructure/data_storage/#summary","title":"Summary","text":"<p>Data storage isn\u2019t just an afterthought in Ranger\u2014it\u2019s part of the loop. Logs, prompts, completions, and test results give administrators and developers visibility into how the system behaves and evolves. Whether you use Baserow or another backend, maintaining this data gives you:</p> <ul> <li>Accountability</li> <li>Audit trails</li> <li>Insight for improvement</li> <li>Historical baselines for performance comparison</li> </ul> <p>Use storage to observe, understand, and refine Ranger's real-world behavior.</p>"},{"location":"infrastructure/mcp/","title":"Model Context Protocol (MCP)","text":"<p>Ranger uses the Model Context Protocol (MCP) to bridge the gap between natural language processing and real-world tool execution. MCP enables Ranger to safely and flexibly invoke external APIs, tools, and services by standardizing how actions are defined, described, and executed from within an AI-driven system.</p>"},{"location":"infrastructure/mcp/#what-is-mcp","title":"What Is MCP?","text":"<p>MCP is a protocol for connecting language models to tools. Originally introduced by Anthropic in 2024, MCP defines a standardized way for an LLM to understand what functions are available, what arguments they require, and how to invoke them safely.</p> <p>In Ranger, MCP acts as the intermediary between the AI and the cyber range\u2019s various services (e.g., GHOSTS, range deployment tools, n8n workflows, etc.).</p>"},{"location":"infrastructure/mcp/#why-mcp","title":"Why MCP?","text":"<ul> <li>Modularity: You can integrate any tool or service by wrapping it in an MCP-compliant server.</li> <li>Interoperability: The LLM doesn\u2019t need to know anything about the service\u2019s internal API \u2013 it only sees a generic function description.</li> <li>Safety: Tools expose only explicitly defined functions, with input validation, descriptions, and roles.</li> <li>Future-proofing: As services evolve, their MCP wrapper can be updated without modifying Ranger\u2019s core logic or LLM prompts.</li> </ul>"},{"location":"infrastructure/mcp/#how-it-works","title":"How It Works","text":"<ol> <li>Tool Description: Each tool is exposed through an MCP server that describes available functions, including:</li> <li>Name</li> <li>Description</li> <li>Parameters (with types and descriptions)</li> <li> <p>Optional role-based access</p> </li> <li> <p>LLM Planning: Ranger\u2019s LLM receives this tool metadata as part of the system prompt or RAG. When a user gives an instruction, the LLM can select a function and generate a structured function call.</p> </li> <li> <p>Execution Flow:</p> </li> <li>Ranger parses the LLM\u2019s response to check for a function call.</li> <li>The request is forwarded to the appropriate MCP server.</li> <li>The MCP server executes the function and returns the result.</li> <li>Ranger relays the result back to the user via chat or logs.</li> </ol>"},{"location":"infrastructure/mcp/#example","title":"Example","text":"<p>User says:</p> <p>\u201cSet up 10 NPCs browsing news sites on subnet 10.0.0.0/24\u201d</p> <p>Ranger: - Uses MCP metadata to identify a <code>deploy_npcs</code> function from the Ghosts MCP server. - Sends a JSON call like:   <pre><code>{\n  \"function\": \"deploy_npcs\",\n  \"parameters\": {\n    \"count\": 10,\n    \"activity\": \"news_browsing\",\n    \"subnet\": \"10.0.0.0/24\"\n  }\n}\n</code></pre> - Receives confirmation or result, and informs the user.</p>"},{"location":"infrastructure/mcp/#adding-a-new-mcp-service","title":"Adding a New MCP Service","text":"<ol> <li>Define your service\u2019s actions in a JSON schema or code.</li> <li>Wrap the service in an MCP server (we provide templates).</li> <li>Deploy the server and register its URL with Ranger.</li> <li>Optionally, update the Ranger system prompt with a summary of the new tool\u2019s capabilities.</li> </ol>"},{"location":"infrastructure/mcp/#security-considerations","title":"Security Considerations","text":"<ul> <li>Each MCP function can include access controls (e.g., admin-only).</li> <li>Ranger should verify the user\u2019s role or session before invoking sensitive functions.</li> <li>Always validate inputs on the MCP server side.</li> </ul>"},{"location":"infrastructure/mcp/#deployment-notes","title":"Deployment Notes","text":"<ul> <li>MCP servers are typically small FastAPI or Flask apps (we use FastMCP).</li> <li>They can run as containers and communicate over internal networks.</li> <li>Ranger maintains a registry of available MCP services and queries them as needed.</li> </ul> <p>MCP enables Ranger to function like an intelligent operator\u2014reading user intent, deciding which tools to use, and taking action\u2014without hardcoding tool-specific logic into the LLM or application core.</p>"},{"location":"infrastructure/rag/","title":"RAG","text":""},{"location":"infrastructure/rag/#rag-and-vector-databases-for-knowledge-retrieval","title":"RAG and Vector Databases for Knowledge Retrieval","text":"<p>To extend Ranger\u2019s intelligence with up-to-date, exercise-specific knowledge, we implement Retrieval-Augmented Generation (RAG) using a vector database\u2014specifically, Qdrant. This allows Ranger to overcome the context limitations of large language models by injecting precise, relevant information into the LLM\u2019s prompt at runtime.</p>"},{"location":"infrastructure/rag/#why-qdrant","title":"Why Qdrant?","text":"<p>Qdrant is an open-source vector search engine optimized for high-performance similarity search on embedding vectors. It\u2019s containerized, easy to deploy in air-gapped environments, and supports both REST and gRPC interfaces. We selected it for:</p> <ul> <li>Purpose-built similarity search</li> <li>Low latency, high throughput</li> <li>Simple API surface</li> <li>Full control over data (self-hosted, no external dependencies)</li> </ul> <p>By default, we expose Qdrant on port <code>6333</code>. The Ranger API communicates with Qdrant to perform search queries and maintain the vector index.</p>"},{"location":"infrastructure/rag/#what-goes-into-qdrant","title":"What Goes into Qdrant?","text":"<p>We store environment-specific data as embeddings:</p> <ul> <li>Network diagrams and topologies</li> <li>Standard Operating Procedures (SOPs)</li> <li>Scenario configurations</li> <li>Exercise briefs and reports</li> </ul> <p>These documents are preprocessed into chunks, embedded into vector space (using the same embedding model used at runtime), and stored in Qdrant collections.</p>"},{"location":"infrastructure/rag/#how-retrieval-works","title":"How Retrieval Works","text":"<p>When a user asks something like:</p> <p>\u201cHas Team Alpha achieved objective X in the last run?\u201d \u201cShow me the network topology for this exercise.\u201d</p> <p>Ranger embeds the user\u2019s question, queries Qdrant for similar embeddings, and retrieves the most relevant document snippets. These are injected into the LLM prompt to inform the response.</p> <p>This retrieval pipeline typically looks like:</p> <ol> <li>Embed user question</li> <li>Query Qdrant for top-N similar vectors</li> <li>Retrieve text chunks from indexed documents</li> <li>Inject retrieved text into system prompt context</li> <li>Generate final response from the LLM</li> </ol>"},{"location":"infrastructure/rag/#updating-the-index","title":"Updating the Index","text":"<p>We keep Qdrant\u2019s index updated with current exercise materials before each run. For example:</p> <ul> <li>Loading scenario briefs and attacker profiles</li> <li>Ingesting updated range topology and configurations</li> </ul> <p>External integrators can automate this process or extend it to new data sources. For example, if your environment includes a proprietary CMDB or scoring engine, you could feed relevant documents or data extracts into Qdrant.</p>"},{"location":"infrastructure/rag/#design-trade-offs","title":"Design Trade-Offs","text":"<p>Adding RAG adds complexity\u2014data must be maintained, embedded, and indexed. But the benefit is substantial: Ranger becomes context-aware, responsive to dynamic scenarios, and capable of answering detailed operational queries using your environment\u2019s ground truth.</p> <p>In short, Qdrant gives Ranger memory.</p>"},{"location":"infrastructure/ranger_api/","title":"Ranger Core API","text":"<p>The Ranger Core API is the main service endpoint that handles incoming requests, coordinates system components, and integrates with a language model backend. It does not contain the AI model itself \u2014 rather, it serves as the central controller that routes messages, invokes tools, and returns results. It sits between client applications and the LLM, acting as a mediator. It can simply relay messages to the LLM for language understanding, or it can execute complex workflows by calling external tools and APIs while incorporating LLM output anywhere within the workflow.</p>"},{"location":"infrastructure/ranger_api/#role-in-the-system","title":"Role in the System","text":"<p>When a user interacts with Ranger (for example, via a chat interface like Open-WebUI), the message is sent to the Ranger Core API. This API determines whether the message is informational (just needs a model response) or operational (requires triggering a tool or action). For language understanding and response generation, the Core API uses an LLM service such as Ollama, OpenAI, or Claude.</p> <p>If tool use is needed, the API executes a workflow, which might incorporate other functionalities such as RAG, Model Context Protocol (MCP) or otherwise to identify capabilities and call the right functions.</p>"},{"location":"infrastructure/ranger_api/#model-context-protocol-mcp","title":"Model Context Protocol (MCP)","text":"<p>MCP is an open standard introduced by Anthropic in late 2024 to simplify how LLM-enabled applications interact with external tools and APIs. Instead of hardwiring logic for every service, MCP provides a generic interface where:</p> <ul> <li>The Core API queries connected MCP servers to discover available functions</li> <li>The LLM receives these function definitions in its prompt</li> <li>The LLM can return a function call with arguments</li> <li>The Core API executes that call through the MCP server</li> <li>The result is returned to the user, optionally with additional reasoning</li> </ul> <p>This flow \u2014 Plan \u2192 Function Call \u2192 Execute \u2192 Respond \u2014 is the core loop of Ranger's intelligent behavior.</p>"},{"location":"infrastructure/ranger_api/#adding-new-tools","title":"Adding New Tools","text":"<p>Each external system (e.g., GHOSTS, n8n, PCTE APIs) can be wrapped in its own MCP server. These servers expose available functions in a machine-readable format. The Ranger Core API doesn\u2019t need to change to support new tools \u2014 it simply reads what each MCP server exposes.</p> <p>This modularity makes the system extensible and future-proof. If you need Ranger to support a new service, you just write an MCP wrapper for it.</p>"},{"location":"infrastructure/ranger_api/#summary","title":"Summary","text":"<ul> <li>The Ranger Core API is the central orchestrator, not the AI itself</li> <li>It routes messages between chat front-ends, LLMs, and tool integrations</li> <li>It uses MCP to dynamically discover and call tools</li> <li>It enables intelligent, language-based control of complex range operations</li> </ul>"},{"location":"infrastructure/user_interface/","title":"Client Interface","text":""},{"location":"infrastructure/user_interface/#user-interface","title":"User Interface","text":"<p>Ranger is most easily accessed through a web-based chat interface currently powered by Open-WebUI. This self-hosted, offline-capable chat frontend offers a robust user experience out-of-the-box, providing multi-user support, access controls, and session management\u2014ideal for cyber range environments with multiple users.</p>"},{"location":"infrastructure/user_interface/#why-open-webui","title":"Why Open-WebUI?","text":"<p>We chose Open-WebUI instead of building a custom frontend because it delivers:</p> <ul> <li>Multi-user accounts and role-based access</li> <li>Persistent chat sessions and history</li> <li>Response feedback and evaluation tracking</li> <li>Admin dashboard with conversation and user management</li> <li>Containerized deployment</li> </ul> <p>All these features make it especially useful in exercise environments where several users interact with Ranger concurrently.</p>"},{"location":"infrastructure/user_interface/#integration-with-ranger","title":"Integration with Ranger","text":"<p>Open-WebUI is typically used to communicate directly with an LLM backend (e.g., Ollama). In our design, however, we route Open-WebUI\u2019s traffic to the Ranger API instead. To enable this, Ranger mimics the OpenAI chat completions API, including:</p> <ul> <li>JSON request/response formats</li> <li>Streaming partial responses (for real-time feedback)</li> <li>Chat memory and multi-turn dialogue support</li> </ul> <p>This allows Ranger to intercept messages, decide whether to answer using the LLM or to perform an action, and stream responses back to the frontend\u2014all transparently to the user.</p> <p>For example, when a user says:</p> <p>\"Have Sally browse her financial sites to find out more about her investments\"</p> <p>Open-WebUI sends that prompt to Ranger\u2019s API. Ranger processes it, determines tool usage (like telling GHOSTS NPCs to perform some activity), and streams back both status updates and final results\u2014all while the user remains in a familiar chat UI.</p>"},{"location":"infrastructure/user_interface/#admin-features","title":"Admin Features","text":"<p>Open-WebUI includes a built-in admin panel:</p> <ul> <li>Manage user accounts</li> <li>View and export conversations</li> <li>Adjust model and system prompt settings</li> <li>Upload documents and templates (optional)</li> </ul> <p>It also supports extensions like Pipelines\u2014allowing Python scripts to be triggered on chat events that are outside the scope of Ranger for now, but which are possible.</p>"},{"location":"infrastructure/user_interface/#flexibility-and-future-compatibility","title":"Flexibility and Future Compatibility","text":"<p>Ranger\u2019s adherence to the OpenAI-compatible API schema ensures that it can work with other chat frontends as well. If Open-WebUI no longer suits your needs, swapping to another interface (like Chatbot UI or even a CLI wrapper) would require minimal changes.</p> <p>For now, Open-WebUI strikes the right balance of usability, control, and extensibility\u2014making it an ideal interface for Ranger deployments in cyber range environments.</p>"},{"location":"infrastructure/workflows/","title":"Workflows","text":"<p>Ranger integrates with workflow automation tools to perform multi-step operations, trigger external systems, and manage side effects beyond its core logic. Our current integration uses n8n, an open-source workflow engine designed for extensibility, visual editing, and self-hosted operation.</p>"},{"location":"infrastructure/workflows/#why-n8n","title":"Why n8n?","text":"<p>We selected n8n because it offers:</p> <ul> <li>Drag-and-drop visual workflow editor</li> <li>Hundreds of built-in integrations (HTTP, databases, messaging, cloud services)</li> <li>Secure, local deployment</li> <li>Trigger-based automation with fine-grained control</li> </ul> <p>This makes it well-suited for complex cyber range workflows, such as sending alerts, recording outcomes, or orchestrating actions across multiple tools.</p>"},{"location":"infrastructure/workflows/#how-it-works","title":"How It Works","text":"<p>Ranger communicates with n8n in two primary ways:</p> <ul> <li>Direct HTTP Call: Ranger's API can call a webhook defined in n8n, passing parameters (e.g., { \"user\": \"Alpha\", \"event\": \"trigger_ransomware\" }).</li> <li>Chat-Triggered Execution: Through Open-WebUI\u2019s Pipelines system, chat events (like a specific user prompt) can trigger predefined n8n workflows without modifying Ranger\u2019s core logic.</li> </ul>"},{"location":"infrastructure/workflows/#use-cases","title":"Use Cases","text":"<ul> <li>Log exercise milestones (e.g., \u201cTeam Bravo achieved Objective X\u201d)</li> <li>Send alerts when certain events occur</li> <li>Schedule post-exercise cleanup</li> <li>Log tool usage to external dashboards</li> <li>Dynamically configure virtual environments based on scenario state</li> </ul>"},{"location":"infrastructure/workflows/#security-and-deployment","title":"Security and Deployment","text":"<p>We deploy n8n in a separate container, typically on port 5678, and secure it behind internal access controls. Workflows triggered by Ranger require no public endpoint and are fully self-contained. Authentication and access restrictions are configurable per use case.</p>"},{"location":"infrastructure/workflows/#extending-the-system","title":"Extending the System","text":"<p>While we use n8n today, Ranger\u2019s architecture is not tied to it. Any HTTP-based automation system can be integrated similarly\u2014Node-RED, Zapier (offline-compatible clone), or even custom Flask apps. Ranger simply needs a known endpoint and a JSON schema.</p>"}]}